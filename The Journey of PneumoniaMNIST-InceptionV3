# ğŸ¬ The Journey of PneumoniaMNIST-InceptionV3

Welcome to the behind-the-scenes story of my work with the **PneumoniaMNIST dataset**! This is where I dove into the nitty-gritty of building and tweaking a model, learning from mistakes, and dreaming up improvements. Letâ€™s walk through it step by step!

---

## ğŸ“Š Kicking Off with the Dataset

I began with the **PneumoniaMNIST dataset**:
- 3,882 training
- 524 validation
- 624 test images

Class distribution:
- 1,134 Normal
- 2,748 Pneumonia

The imbalance jumped out at me right away â€” more Pneumonia than Normal! This set the stage for some creative problem-solving.

---

## ğŸ› ï¸ Building the Model

I picked **InceptionV3**, pre-trained on ImageNet, and fine-tuned its last **50 layers**.

To keep things robust, I added:
- Dense layers: **128 and 64 units** with ReLU activation
- **L2 regularization**, **batch normalization**, and **dropout (0.5, 0.3)** to tame overfitting

I thought this setup would handle the complexity, but the real test was balancing the data.

---

## âš–ï¸ Balancing the Scales

To tackle the imbalance, I oversampled the **Normal** class 3x, generating ~1,152 images (total **5,034**).

Then, I spiced it up with **augmentations**:
- Flipping
- Brightness adjustments (Â±0.1)
- Contrast (0.8â€“1.2)
- Hue tweaks (Â±0.05)
- Rotation (0â€“360Â°)
- Zoom (0.9â€“1.1)
- Random crop

It felt like giving the model a creative toolkit!

---

## â±ï¸ Training Trials

I trained the model in **two phases**:
- **Initial Training**: 11 epochs with `EarlyStopping` (patience = 7)
- **Fine-tuning**: 11 more epochs at learning rate **5e-7** (patience = 5)

**Results**:
- **Accuracy**: 80.61%
- **AUC**: 0.9190
- **Normal Recall**: 48%
- **Pneumonia Recall**: 96%

âš ï¸ I over-optimized for Pneumonia, and Normal cases lagged behind.

---

## ğŸš§ Roadblocks and Lessons

It wasnâ€™t all smooth sailing!

- ğŸ›‘ **FileNotFoundError** because I forgot to update the path from `/boot/XRAY DATA` â  
  ğŸ”‘ *Lesson: Always test file paths early!*

- â— **Version conflicts** with `scikit-learn` and `imbalanced-learn` â  
  ğŸ”§ *Fix: Pin versions like `scikit-learn==1.4.2` and `imbalanced-learn==0.10.1`*

- â³ **Rushed near July 9, 2025** â  
  ğŸ“… *Lesson: Manage your time! Hyperparameter tuning needs breathing room.*

---

## ğŸ’¡ Next Steps and Reflections

Looking back, Iâ€™m planning to refine the model with:

- âœ… **2x oversampling (~768 Normal images)** + **focal loss** to improve Normal recall
- â³ **Longer fine-tuning**: ~20 epochs at a lower learning rate (3e-7)
- ğŸ” **Grad-CAM visualizations** to interpret model predictions

---

Each stumble taught me something new. Iâ€™m excited to keep tweaking and pushing this work forward ğŸš€
